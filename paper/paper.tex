% document type
\documentclass[12pt]{article}

% packages
\usepackage{amsmath, amsfonts, amssymb}
\usepackage{algorithm}
\usepackage{algpseudocode}
\usepackage{hyperref}
\usepackage{geometry}
\usepackage{bookmark}

\usepackage{tikz} % for graphics
\usetikzlibrary{arrows,shapes}

% title and author
\title{Numeric to Symbolic to Parametric space of solutions of Polynomial Systems of Equations}
\author{Ilay Menahem, Michael Shalyt, Ido Kaminer}

% article text
\begin{document}

\maketitle

% abstract
\begin{abstract}
    There is a problem shared among many fields of science of solving polynomial systems of equations.
    The required solution is often a space of solutions, rather than a single solution.
    Applications include QFT, robotics, cryptography, optimization, chemistry, electrical engineering, and more. [we need other examples... best is to have concrete examples from history. If not then maybe say generally that in many areas of science and engineering a parametric solution can surprise us and help where our creativity is limited].

    This paper presents a scalable pipeline for finding a space of solutions (of full dimension) to polynomial systems of equations.
    We start from numerical solutions, transitioning to symbolic solutions, and finally parametrize the solution space via structured exploration.
    We then also discuss other complementary methods and discuss what open questions still remain.
    We demonstrate these methods on concrete examples from the field of QFT. 
\end{abstract}

\section*{Introduction}
Solving polynomial systems of equations is a fundamental problem in mathematics and computer science, with applications in fields such as robotics, cryptography, optimization, QFT, chemistry, electrical engineering, economics and more.
In QFT, polynomial systems arise in the context of generilizing and unifying theories (e.g. quantum gravity). 
In chemistry, for example, polynomial systems appear in the study and design of reaction chains.
In robotics, for example, polynomial systems are used to model kinematics and motion planning.

In most of these use cases, the required solution is often a space of solutions, rather than a single solution.
Specifically, in QFT the proposed theory must follow gauge invariance, which leads to a space of solutions, and of these solutions one may want to find the one represents the simplest theory.
In chemistry, reaction chains often have multiple valid pathways, leading to a solution space.
In robotics, the solution space may represent all possible configurations of a robotic arm that satisfy certain constraints. one would want to work with the entire solution space to find optimal paths and configurations.
This type of problem is universal across many fields of science and engineering. It calls for a comprehensive approach that can efficiently find and represent the solution space of polynomial systems.

% classic methods for solving polynomial systems of equations
Classic methods for solving polynomial systems of equations include Groebner bases [], resultants [], and homotopy continuation []. However, these methods can be computationally expensive and may not scale well with the size of the system. Other approaches, such as gradient [], and SMT solvers [] are focused on finding specific solutions rather than exploring the entire solution space.

% overview of the proposed pipeline
In this paper, classify/group/review/discuss specially made approaches for finding the general ... in efficient ways relevant for.
We also demonstrate the methods on concrete examples from QFT.
Among those methods, we propose a pipeline that leverages numerical methods to find initial solutions, then refines these solutions into symbolic forms, and finally explores a space of solutions. This approach aims to combine the efficiency of numerical methods, and some insights we may have at how the symbolic solutions may look like, to provide a comprehensive understanding of a solution space of polynomial systems.
We also present and analyze a complementary examples from QFT, showcasing the effectiveness of our approach in practical scenarios.
We finally outline the open questions and future directions [more concrete] in this field.


\section*{Mathematical Background}
Let $\mathbb{K}[\mathbf{x}]$ denote the polynomial ring in variables $\mathbf{x} = (x_1, \ldots, x_n)$ over a field $\mathbb{K}$, typically $\mathbb{R}$ or $\mathbb{C}$. A polynomial system is defined by a map $F: \mathbb{K}^n \to \mathbb{K}^m$ with components $f_1, \ldots, f_m \in \mathbb{K}[\mathbf{x}]$. The locus of simultaneous solutions constitutes the \textit{affine variety} $V(F) = \{ \mathbf{a} \in \mathbb{K}^n \mid F(\mathbf{a}) = \mathbf{0} \}$.

\noindent given a  continuously differentiable system of equations $F\in C_1$ (possibly polynomial) with a solution point $p$, the dimension of the solution space in the neighborhood of $p$ is $n - rank(J_F(p))$ where $J$ is the Jacobian matrix of $F$ at point $p$ \cite{gatilov2014using}.


\section*{What is a solution?}
there are a few ways to interpret the meaning of the solution to a system of equations.
\begin{list}{-}{}
    \item \textbf{numerical solutions:} specific numerical values for the variables that satisfy all equations in the system (with very high precision).
    \item \textbf{symbolic solutions:} symbolic expressions of constants that satisfy all equations in the system.
    \item \textbf{lowest norm solution:} a solution numerical or symbolic, that minimizes some norm. for example, $L_0$ norm minimization leads to the sparsest solution, while $L_2$ norm minimization leads to the solution with the smallest Euclidean length.
    \item \textbf{parametric expression of a variety:} a set of symbolic or numerical approximated expressions, that provide a parametric representation of the set of all possible solutions to the system. the set of system solutions is also known as the variety. there are various ways to represent the variety parametrically, such as rational, polynomial, or trigonometric parametrizations. yet there are some varieties that cannot be parametrized such as elliptic curves.
    \item \textbf{point cloud sampling of the variety:} a large set of numerical or symbolic solutions that sample the variety, which can be used to understand its structure and properties.
\end{list}


\section*{Moving between types of solutions}
one can note that it is fairly straightforward to find certain types of solutions such as numerical solutions via Newton-Raphson, but sometimes it is more desirable to have symbolic solutions or parametric expressions of the variety. in the diagram below, we illustrate the different types of solutions and the methods we will use to move between them.

\begin{tikzpicture}[node distance=2cm]
% spread out layout
    \node (start) at (6,0) {Polynomial System of Equations};
    \node (numerical) at (0,-3) {Numerical Solution};
    \node (symbolic) at (8,-3) {Symbolic Solution};
    \node (num_cloud) at (0,-7) {Numerical Point Cloud};
    \node (sym_cloud) at (6,-7) {Symbolic Point Cloud};
    \node (parametric) at (12,-7) {Space of Solutions};
    \node (num_parametric) at (4,-10) {Numerical Parametric};

    % Straight arrows (no bends)
    \draw[->] (start) -- (numerical) node[midway, left] {(6, 7, 8)};
    \draw[->] ([yshift=6pt]numerical.east) -- ([yshift=6pt]symbolic.west) node[midway, above] {(1)};
    \draw[->] ([yshift=-6pt]symbolic.west) -- ([yshift=-6pt]numerical.east) node[midway, below] {(4)};
    \draw[->] ([xshift=6pt]numerical.south) -- ([xshift=6pt]num_cloud.north) node[midway, left] {(2)};
    \draw[->] ([yshift=6pt]symbolic.south) -- ([yshift=6pt]sym_cloud.north) node[midway, right] {(2)};
    \draw[->] ([xshift=-6pt]num_cloud.north) -- ([xshift=-6pt]numerical.south) node[midway, right] {(5)};
    \draw[->] ([yshift=-6pt]sym_cloud.north) -- ([yshift=-6pt]symbolic.south) node[midway, left] {(5)};
    \draw[->] ([yshift=6pt]num_cloud.east) -- ([yshift=6pt]sym_cloud.west) node[midway, below] {(1)};
    \draw[->] ([yshift=-6pt]sym_cloud.west) -- ([yshift=-6pt]num_cloud.east) node[midway, above] {(4)};
    \draw[->] ([yshift=6pt]sym_cloud.east) -- ([yshift=6pt]parametric.west) node[midway, right] {(3)};
    \draw[->] ([yshift=-6pt]parametric.west) -- ([yshift=-6pt]sym_cloud.east) node[midway, left] {(5)};
    \draw[->] (symbolic) -- (parametric) node[midway, above right] {(2)};
    \draw[->] (parametric) -- (num_parametric) node[midway, right] {(4)};
    \draw[->] ([yshift=6pt]num_parametric.north) -- ([yshift=6pt]num_cloud.south) node[midway, below left] {(5)};
    \draw[->] ([yshift=-6pt]num_cloud.south) -- ([yshift=-6pt]num_parametric.north) node[midway, above right] {(3)};
\end{tikzpicture}

\begin{enumerate}
    \item \textbf{PSLQ and numerical fitting:} in this method, we use the PSLQ algorithm to find integer relations in the numerical solutions, replace numerical results with symbolic expressions, refit the solution and repeat until we have replaced all numerical values with symbolic expressions.
    \item \textbf{Going penpendicular to Jacobian null space:} given a solution to the polynomial system of equations, and if we will go in a direction that is perpendicular to the Jacobian null space at that point, we will remain in the solution space of the system. by taking small steps in such directions, we can sample more solutions to the system.
    \item \textbf{Symbolic regression:} given a point cloud of solutions, we can use symbolic regression to find parametric expressions that fit the point cloud.
    \item \textbf{Evaluating symbolic expressions:} given symbolic expressions, we can evaluate them to get numerical solutions.
    \item \textbf{Sampling points:} given numerical or symbolic parametric expressions of the variety, we can sample points from them to get point clouds of solutions.
    \item \textbf{SMT solvers:} SMT solvers can sometimes be used to find symbolic or numerical solutions to systems of equations.
    \item \textbf{Newton-Raphson:} given a system of equations, we can use the Newton-Raphson method to find numerical solutions to the system.
    \item \textbf{Homotopy continuation:} Homotopy continuation methods take a system of equations with known solutions and continuously deform it into the target system, tracking the solutions throughout the deformation process to find solutions to the original system.
\end{enumerate}


\section*{Linear elimination}

even in the case of very large non linear polynomial systems of equations, it is possible to eliminate variables in a linear fashion, which can significantly reduce the size of the system and the number of variables.

\begin{algorithm}[H]
\caption{Linear elimination by substitution}
\label{alg:linear-elimination}
\begin{algorithmic}[1]
\Require Polynomial system $P=\{p_1,\dots,p_k\}\subseteq \mathbb{K}[x_1,\dots,x_m]$
\Ensure Reduced system $Q$ where no $q\in Q$ is linear in any remaining variable; and a substitution map $\Sigma$ to recover eliminated variables
\State $Q \gets P$
\State $V \gets \{x_1,\dots,x_m\}$
\State $\Sigma \gets \emptyset$
\State $L \gets \{(x,p)\mid x\in V,\ p\in Q,\ p \text{ is linear in } x\}$
\State interreduce $Q$ \Comment{optional step to simplify the system}
\State make all the polynomials in $Q$ monic \Comment{optional step to simplify the system}
\While{$L \neq \emptyset$}
    \State Select any pair $(x,p)\in L$
    \State $L \gets L\setminus\{(x,p)\}$
    \State $Q \gets Q\setminus\{p\}$ \Comment{use $p$ to eliminate $x$}
    \State Solve $p=0$ for $x$ to obtain $x \gets f(\hat{x})$ \Comment{$\hat{x}=V\setminus\{x\}$}
    \State $\Sigma \gets \Sigma \cup \{x \mapsto f(\hat{x})\}$
    \ForAll{$q \in Q$}
        \State $q \gets q[x \leftarrow f(\hat{x})]$
    \EndFor
    \State $V \gets V\setminus\{x\}$
    \State $L \gets \{(x',p')\mid x'\in V,\ p'\in Q,\ p' \text{ is linear in } x'\}$
    \State interreduce $Q$ \Comment{optional step to simplify the system}
\EndWhile
\State \Return $(Q,\Sigma)$
\end{algorithmic}
\end{algorithm}

the polynomial order we will use to interreduce, and make polynomials monic, should maximize the number of monomials with that are linear in one of any of the variables. thus we should use the grevlex order. we have the freedom to choose which linear variable to eliminate at each step. good heuristics to choose which variable to eliminate include:
\begin{list}{-}{}
    \item \textbf{Lowest degree in the other equations:} eliminate the variable that appears with the lowest degree in the other equations first.
    \item \textbf{Lowest degree after substitution:} eliminate the variable that will lead to the lowest degree in the other equations after substitution.
    \item \textbf{Greatest or least number of occurrences:} eliminate the variable that appears in the greatest or least number of equations first.
\end{list}

\section*{Numerical solutions via Newton-Raphson}
to apply the Newton-Raphson method to an underdetermined system of equations we can use the following update step\cite{BenIsrael1966ANM}: \[x_{n+1} = x_n - J^+(x_n) F(x_n)\] where $J^+$ is the Moore-Penrose pseudoinverse of the Jacobian matrix $J$ of $F$ at point $x_n$.

% minimizing the $L_1$ norm to improve solution sparsity and stability of Newton-Raphson
it is well known that minimizing the $L_1$ norm of the solution can lead to sparser solutions [], and can also improve the stability of the Newton-Raphson method []. % write more about methods for sparsity and stability

\section*{Symbolic solutions via rational approximation and homepoty continuation}
if we have a numerical solution in neighborhood of solutions of dimension $k$, one of the simplest ways to convert it to a parametric solution would to covert some of the entries of the numerical solution to symbolic fractions of close values. the following theorem formalizes this idea.
\\\textbf{theorem:} let there be a $k$ dimensional manifold (can be a solution space of a polynomial system of equations) embedded in an $n$ dimensional space. there is a point in the manifold, such that $k$ of the $n$ coordinates are rational.
\\\textbf{proof:} 

\section*{Parametric solutions via linear elimination and rationalization}
there are two main reasons that linear elimination would stop finding variables to eliminate without finishing: 

\begin{list}{-}{}
    \item Substituting the parametric expression of the variable leads to a very high degree polynomial, which is non linear in all variables.
    \item there is an equation that would be linear in a variable, if we will zero some other variable.
\end{list}

\begin{algorithm}[H]
\caption{Parametric solution via linear elimination and rationalization}
\begin{algorithmic}[1]
\Require Polynomial system $P=\{p_1,\dots,p_k\}\subseteq \mathbb{K}[x_1,\dots,x_m]$
\Ensure Parametric solution $Q$ with some variables expressed symbolically in terms of free variables
    \State $Q \gets P$
    \State $V \gets \{x_1,\dots,x_m\}$
    \While{true}
        \State $(Q,\Sigma) \gets$ LinearElimination$(Q)$ \Comment{Algorithm \ref{alg:linear-elimination}}
        \State find which variables can be zeroed \Comment{use Newton-Raphson, and use Homotopy continuation to try to zero variables in the solution}
        \State find which variables can be rationalized \Comment{same as zeroing but instead of zeroing, find the closest low demnoator rational number}
        \If{variables can be zeroed}
            \State choose variable to zero $z \in V$
            \State record in $\Sigma$ that $z \mapsto 0$
        \ElsIf{variables can be rationalized}
            \State choose variable to rationalize $z \in V$
            \State record in $\Sigma$ that $z \mapsto \text{rational\_approx}(z)$
        \Else
            \State \textbf{break}
        \EndIf
        \State $Q \gets Q[z \leftarrow \Sigma(z)]$
        \State $V \gets V\setminus \{z\}$
    \EndWhile
    \State \Return $(Q,\Sigma)$
\end{algorithmic}
\end{algorithm}

to choose which variable to zero or rationalize, one can use heuristics such as:
\begin{list}{-}{}
    \item \textbf{Most linear variables after substitution:} choose the variable that will lead to the most equations being linear in some variable after substitution.
\end{list}


\section*{exploration of space of solutions}
here we present a theorem regarding given one solution how can we derive a neighborhood of solutions to any system of equations via the Jacobian null space at that point.
\\\textbf{theorem:} let $F = \{f_1, f_2, \ldots, f_n\}$ be a system of smooth functions in variables $x = (x_1, x_2, \ldots, x_m)$. let $J$ be the Jacobian matrix of $F$ with respect to $x$, $x_0$ be a point such that $F(x_0) = 0$, and $v \in N(J(x_0))$ be a vector in the null space of $J$ at point $x_0$. 
then $\frac{d}{dt}F(x_0 + tv)|_{t=0} = 0$.
\\\textbf{proof:} By the multivariable chain rule, the derivative of the composition $F(x_0 + tv)$ with respect to $t$ is given by:
\[\frac{d}{dt}F(x_0 + tv) = J(x_0 + tv) \cdot \frac{d}{dt}(x_0 + tv) = J(x_0 + tv) \cdot v\] 
Evaluating this expression at $t=0$: \[\frac{d}{dt}F(x_0 + tv)\bigg|_{t=0} = J(x_0) \cdot v\]
Since $v$ is in the null space of $J(x_0)$, we have $J(x_0)v = 0$. Thus: \[ \frac{d}{dt}F(x_0 + tv)\bigg|_{t=0} = 0 \]

a use of this theorem to extract a parametric solution from a symbolic one is in the appendix. 

\subsection*{Numerical exploration of space of solutions}

\subsection*{Symbolic exploration of space of solutions}

\section*{Finding the whole space of solutions}
After parametrizing a set of solutions, one may want to ensure that the entire solution space has been found. For that one can Rabinowitsch Trick to add an additional constraint to the system of equations that excludes the already found solutions, and then attempt to find new solutions to the modified system. If no new solutions are found, it can suggest that the entire solution space has been explored.

% A simple example of applying this method is provided in the appendix.

\section*{Appendix}
\subsection*{Derivation of neighborhood of solutions via Jacobian null space}
system of polynomial equations:
\begin{align*}
f_1 &= x^2 + y^2 + z^2 - 1 = 0 \\
f_2 &= x + y + z - 1 = 0
\end{align*}
Let us choose a solution $x_0 = (1, 0, 0)$. We can verify that $f_1(1,0,0) = 1^2 + 0 + 0 - 1 = 0$ and $f_2(1,0,0) = 1 + 0 + 0 - 1 = 0$.

The Jacobian matrix $J$ is given by:
\[
J = \begin{pmatrix}
\frac{\partial f_1}{\partial x} & \frac{\partial f_1}{\partial y} & \frac{\partial f_1}{\partial z} \\
\frac{\partial f_2}{\partial x} & \frac{\partial f_2}{\partial y} & \frac{\partial f_2}{\partial z}
\end{pmatrix} = \begin{pmatrix}
2x & 2y & 2z \\
1 & 1 & 1
\end{pmatrix}
\]
Evaluating $J$ at $x_0 = (1, 0, 0)$:
\[
J(x_0) = \begin{pmatrix}
2 & 0 & 0 \\
1 & 1 & 1
\end{pmatrix}
\]
To find the null space, we solve $J(x_0)v = 0$ for $v = (v_x, v_y, v_z)^T$:
\[
\begin{pmatrix}
2 & 0 & 0 \\
1 & 1 & 1
\end{pmatrix} \begin{pmatrix}
v_x \\ v_y \\ v_z
\end{pmatrix} = \begin{pmatrix}
0 \\ 0
\end{pmatrix}
\]
From the first row, $2v_x = 0 \implies v_x = 0$.
From the second row, $v_x + v_y + v_z = 0 \implies 0 + v_y + v_z = 0 \implies v_y = -v_z$.
Let $v_z = 1$, then $v_y = -1$. Thus, a vector in the null space is $v = (0, -1, 1)^T$.

Moving from $x_0$ in the direction of $v$ keeps the system approximately solved to the first order. moving the direction of $v$ can be done numerically by choosing a small $t$, evaluating $x(t) = x_0 + tv$, and doing a Newton-Raphson step to refine the solution.

To perform the symbolic walk, we calculate the null space of $J$ symbolically. The null space direction $v(x,y,z)$ must satisfy $J v = 0$, which means $v$ is orthogonal to the gradients of $f_1$ and $f_2$. This direction is given by the cross product of the gradients (ignoring the scalar factor 2 from $\nabla f_1$):
\[
v(x,y,z) = \frac{1}{2}\nabla f_1 \times \nabla f_2 = \begin{pmatrix} x \\ y \\ z \end{pmatrix} \times \begin{pmatrix} 1 \\ 1 \\ 1 \end{pmatrix} = \begin{pmatrix} y-z \\ z-x \\ x-y \end{pmatrix}
\]
This yields the system of differential equations:
\begin{align*}
\dot{x} &= y - z \\
\dot{y} &= z - x \\
\dot{z} &= x - y
\end{align*}
Solving this system with initial condition $x(0) = (1, 0, 0)$ yields the parametric solution for the variety.

Adding the three equations gives $\dot{x} + \dot{y} + \dot{z} = 0$, implying $x+y+z = C_1$. From the initial condition, $1+0+0=1$, so $x+y+z=1$.
Differentiating $\dot{x}$ gives $\ddot{x} = \dot{y} - \dot{z} = (z-x) - (x-y) = z+y-2x$. Substituting $y+z = 1-x$, we get $\ddot{x} = (1-x) - 2x = 1 - 3x$.
This is a linear ODE $\ddot{x} + 3x = 1$. The homogeneous solution is $A\cos(\sqrt{3}t) + B\sin(\sqrt{3}t)$, and the particular solution is $x_p = 1/3$.
So $x(t) = A\cos(\sqrt{3}t) + B\sin(\sqrt{3}t) + 1/3$.
Using $x(0)=1$, we get $A + 1/3 = 1 \implies A = 2/3$.
Using $\dot{x}(0) = y(0)-z(0) = 0$, we get $\sqrt{3}B = 0 \implies B=0$.
Thus $x(t) = \frac{2}{3}\cos(\sqrt{3}t) + \frac{1}{3}$.
By symmetry and the cyclic nature of the equations, the solutions for $y$ and $z$ are phase-shifted or can be derived similarly.
Since $\dot{x} = y-z$ and $y+z = 1-x$, we have a system for $y,z$:
$y-z = -\frac{2}{\sqrt{3}}\sin(\sqrt{3}t)$ and $y+z = 1 - (\frac{2}{3}\cos(\sqrt{3}t) + \frac{1}{3}) = \frac{2}{3} - \frac{2}{3}\cos(\sqrt{3}t)$.
Adding these: $2y = \frac{2}{3} - \frac{2}{3}\cos(\sqrt{3}t) - \frac{2}{\sqrt{3}}\sin(\sqrt{3}t) \implies y(t) = \frac{1}{3} - \frac{1}{3}\cos(\sqrt{3}t) - \frac{1}{\sqrt{3}}\sin(\sqrt{3}t)$.
Subtracting: $2z = \frac{2}{3} - \frac{2}{3}\cos(\sqrt{3}t) + \frac{2}{\sqrt{3}}\sin(\sqrt{3}t) \implies z(t) = \frac{1}{3} - \frac{1}{3}\cos(\sqrt{3}t) + \frac{1}{\sqrt{3}}\sin(\sqrt{3}t)$.

The parametric solution is:
\begin{align*}
x(t) &= \frac{1}{3} + \frac{2}{3}\cos(\sqrt{3}t) \\
y(t) &= \frac{1}{3} - \frac{1}{3}\cos(\sqrt{3}t) - \frac{1}{\sqrt{3}}\sin(\sqrt{3}t) \\
z(t) &= \frac{1}{3} - \frac{1}{3}\cos(\sqrt{3}t) + \frac{1}{\sqrt{3}}\sin(\sqrt{3}t)
\end{align*}

\subsection*{Expanding to the whole space of solutions}

\begin{align*}
    0 = x_0^2 + x_1 + x_2^3 - x_1 * x_4 - 3\\
    0 = 9 * x_0 + x_1^2 + x_3 - x_2 * x_3^4 - 5
\end{align*}

we first note that due to the second equation
\[ x_0 = \frac{5 - x_1^2 - x_3 + x_2 * x_3^4}{9} \]

due to the first equation
\[ x_4 = \frac{x_0^2 + x_1 + x_2^3 - 3}{x_1} = \frac{\left(\frac{5 - x_1^2 - x_3 + x_2 * x_3^4}{9}\right)^2 + x_1 + x_2^3 - 3}{x_1} \]

we will take the solution point where $x_1 = \sqrt{5}, x_2 = 0, x_3 = 0$ which leads to $x_0 = 0$ and $x_4 = -2$.

the Jacobian matrix at this point is:
\[ J = \begin{pmatrix}
2x_0 & 1 - x_4 & 3x_2^2 & 0 & -x_1 \\
9 & 2x_1 & -x_3^4 & 1 - 4x_2x_3^3 & 0
\end{pmatrix} = \begin{pmatrix}
0 & 3 & 0 & 0 & -\sqrt{5} \\
9 & 2\sqrt{5} & 0 & 1 & 0
\end{pmatrix} \]

% This idea needs to be checked, i am unsure that it is a simple process.
The rank of this matrix is 2, so the dimension of the solution space around this point is $5 - 2 = 3$. so we found a 3 dimensional space of solutions around this point.

now we ask what if $x_1 = 0$? the new system is:
\begin{align*}
    0 = x_0^2 + x_2^3 - 3\\
    0 = 9 * x_0 + x_3 - x_2 * x_3^4 - 5
\end{align*}

from the second equation we get:
\[ x_0 = \frac{5 - x_3 + x_2 * x_3^4}{9} \]

the first equation becomes:
\[ 0 = \left(\frac{5 - x_3 + x_2 * x_3^4}{9}\right)^2 + x_2^3 - 3 \]
\[ 0 = (5 - x_3 + x_2 * x_3^4)^2 + 81x_2^3 - 243 \]
\[ 0 = x_2^2 * x_3^8 - 2x_2 * x_3^5 + x_3^2 + 81x_2^3 - 10x_2 * x_3^4 + 10x_3 - 218 \]
this equation is a polynomial of degree 3 in $x_2$, and thus there is a close form solution for $x_2$ in terms of $x_3$. using that fact we found the whole space of solutions in this case.

\bibliographystyle{plain}
\bibliography{references}

\end{document}